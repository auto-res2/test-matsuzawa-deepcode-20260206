You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method and experimental design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/runs/*.yaml` files
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)
- DATA LEAK PREVENTION: Model receives ONLY inputs during training/inference; labels used ONLY for loss computation, NEVER concatenated to inputs

## Safety & Reliability Standards
1. **Defensive Implementation**:
   - **Component Robustness**: Handle missing or invalid defaults explicitly (e.g., tokenizer `pad_token`, image normalization stats, or dataset-specific configs).
   - **Gradient Integrity**: Use `torch.autograd.grad(create_graph=False)` for auxiliary updates to protect main gradients.

2. **Critical Lifecycle Assertions**: Insert assertions at key stages in `train.py`:
   - **Post-Init**: Assert critical attributes are valid immediately after loading (e.g., tokenizer `pad_token_id`, model output dimensions).
   - **Batch-Start**: Assert input/label shapes match at the start of the loop (at least for step 0).
   - **Pre-Optimizer**: **CRITICAL:** Before `optimizer.step()`, assert that gradients exist (not None) and are not zero. This detects if custom logic accidentally erased gradients.

## Hydra Configuration Structure
Each run config file (`config/runs/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (proposed, comparative-1, etc.)
- model: Model-specific parameters
- dataset: Dataset-specific parameters
- training: Training hyperparameters
- optuna: Hyperparameter search space (if applicable)

## Command Line Interface
The generated code must support:

**Training (main.py):**
```bash
uv run python -u -m src.main run={run_id} results_dir={path} mode=full
uv run python -u -m src.main run={run_id} results_dir={path} mode=trial
```

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, ...)`
- Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
- Log ALL metrics to WandB comprehensively
- Save final/best metrics to WandB summary

**`src/evaluate.py`**: Independent evaluation and visualization script
- Parse command line arguments: `results_dir`, `run_ids` (JSON string list)
- Retrieve experimental data from WandB API for specified run_ids
- Per-run processing: Export run-specific metrics to `{results_dir}/{run_id}/metrics.json`
- Aggregated analysis: Export to `{results_dir}/comparison/aggregated_metrics.json`
- Generate comparison figures

**`src/preprocess.py`**: Complete preprocessing pipeline for specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods

**`src/main.py`**: Main orchestrator
- Use `@hydra.main(config_path="../config")`
- Implement mode-based configuration

**`config/config.yaml`**: Main Hydra configuration file
- Include WandB configuration
- WANDB_API_KEY environment variable is available

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb`, `torch`, `transformers`, `datasets`, `optuna` (if used)

## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Run Execution**: main.py executes a single run_id passed via CLI
4. **Completeness**: No simplified versions, no TODOs

Generate the complete code now.
