{
  "open_problems": "Auto-CoT-style pipelines still lack a label-free way to detect \"plausible but ungrounded\" demonstrations: a CoT can be (i) internally consistent under resampling and (ii) stable under paraphrases, yet still be mis-anchored—it solves a subtly different problem than the question actually asks (entity/quantity confusion, implicit assumption injection, or paraphrase drift). This failure mode is especially dangerous in social uses (tutoring/decision support) because the rationale looks coherent and repeatable, so it is likely to be trusted and then copied by in-context learning.\n\nKey gap: How can we estimate whether a candidate demonstration's reasoning is not only stable, but also grounded to the original question semantics, using only the same base LLM and cheap post-processing—no labels, no verifier model, no training?",
  "method": "Cycle-Consistent & Paraphrase-Invariant Reliability Auto-CoT (C3-AutoCoT)\n\nRefine PIR-AutoCoT by adding a third \"human checking\" axis: can I reconstruct the problem I solved from my own explanation?\n\nC3-AutoCoT modifies only the demo construction stage (clustering and downstream prompting remain Auto-CoT).\n\nFor each candidate demo question q:\n1) Re-derive (within-question self-consistency): sample m CoT chains on q → answers A(q). Compute r_sc = max_a freq(a in A(q))/m.\n2) Rephrase (paraphrase invariance): generate p paraphrases with the same LLM; filter paraphrases that drift by requiring the multiset of numbers in q_i to match q. For each q_i sample m chains → answers A(q_i). Compute r_pi = mean_i P(answer=a* | q_i).\n3) Reconstruct (cycle-consistency / grounding): Take a representative chain c* for q that yields a*. Ask the LLM to reconstruct the original word problem given c*: q_hat = f_reconstruct(c*). Compute grounding score r_cc = 0.5·cos_sim(emb(q_hat), emb(q)) + 0.5·I(numbers(q_hat)=numbers(q)).\n\nCombined demo reliability: r = r_sc · r_pi · r_cc.\nAccept demo if r ≥ τ and at least one paraphrase survives the number-preservation filter.",
  "experimental_setup": "Tasks/Datasets: SVAMP (fast arithmetic word problems), GSM8K (stronger but heavier)\n\nModels: google/flan-t5-base (primary), google/flan-t5-large (robustness)\n\nCompared methods (same k demos, same evaluation):\n1) Zero-shot-CoT (no demos)\n2) Auto-CoT (cluster + single chain + heuristics)\n3) RW-AutoCoT (self-consistency demo filter)\n4) PIR-AutoCoT (self-consistency + paraphrase invariance)\n5) C3-AutoCoT (proposed)\n\nProtocol:\n- Demo pool: first 500 train questions\n- Clustering: SentenceTransformer embeddings + k-means (k=8)\n- For each cluster, consider up to T=10 candidates until a demo is accepted\n- Evaluation: N=200 test questions; greedy decoding\n- Seeds: 3 seeds (k-means init + sampling)\n\nHyperparameters:\n- m=4 samples per variant\n- p=2 paraphrases\n- reconstruction samples=1 (deterministic)\n- τ=0.25–0.40",
  "primary_metric": "accuracy",
  "experimental_code": "import re, random\nimport numpy as np\nfrom collections import Counter\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\n\nnum_re = re.compile(r\"-?\\d+(?:\\.\\d+)?\")\n\ndef extract_numbers(s: str):\n    return num_re.findall(s)\n\ndef extract_final_number(text: str):\n    nums = extract_numbers(text)\n    return nums[-1] if nums else None\n\ndef agreement_rate(answers):\n    answers = [a for a in answers if a is not None]\n    if not answers:\n        return 0.0, None\n    c = Counter(answers)\n    maj, ct = c.most_common(1)[0]\n    return ct / len(answers), maj\n\n@torch.inference_mode()\ndef generate_texts(model, tok, prompt, n=1, do_sample=True, temperature=0.7, max_new_tokens=256):\n    inp = tok(prompt, return_tensors='pt', truncation=True).to(model.device)\n    outs = model.generate(\n        **inp,\n        do_sample=do_sample,\n        temperature=temperature if do_sample else None,\n        top_p=0.95 if do_sample else None,\n        num_return_sequences=n,\n        max_new_tokens=max_new_tokens,\n    )\n    return tok.batch_decode(outs, skip_special_tokens=True)\n\nif __name__ == \"__main__\":\n    print(\"C3-AutoCoT implementation scaffold\")\n",
  "expected_result": "On SVAMP (200-example test subset) with flan-t5-base, k=8 demos, m=4, p=2:\n- Auto-CoT: 0.20–0.28 accuracy\n- RW-AutoCoT: 0.23–0.32\n- PIR-AutoCoT: 0.27–0.36\n- C3-AutoCoT (proposed): 0.30–0.40\n\nExpected gains:\n- C3 vs Auto-CoT: +0.07 to +0.12\n- C3 vs PIR-AutoCoT: +0.02 to +0.05\n\nHigher is better.",
  "expected_conclusion": "C3-AutoCoT strengthens Auto-CoT with a cognitively inspired triple-check that better matches human verification: (1) re-derive (self-consistency), (2) rephrase (paraphrase invariance), and (3) reconstruct (cycle-consistency grounding). This targets a sharper and more socially relevant failure mode than PIR alone: demonstrations that are stable yet solve a different problem than asked."
}
