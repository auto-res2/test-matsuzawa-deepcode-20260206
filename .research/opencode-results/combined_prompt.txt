# Research Hypothesis and Experimental Design

## Hypothesis
{
  "open_problems": "Auto-CoT-style pipelines still lack a label-free way to detect \"plausible but ungrounded\" demonstrations: a CoT can be (i) internally consistent under resampling and (ii) stable under paraphrases, yet still be mis-anchored—it solves a subtly different problem than the question actually asks (entity/quantity confusion, implicit assumption injection, or paraphrase drift). This failure mode is especially dangerous in social uses (tutoring/decision support) because the rationale looks coherent and repeatable, so it is likely to be trusted and then copied by in-context learning.\n\nKey gap: How can we estimate whether a candidate demonstration's reasoning is not only stable, but also grounded to the original question semantics, using only the same base LLM and cheap post-processing—no labels, no verifier model, no training?",
  "method": "Cycle-Consistent & Paraphrase-Invariant Reliability Auto-CoT (C3-AutoCoT)\n\nRefine PIR-AutoCoT by adding a third \"human checking\" axis: can I reconstruct the problem I solved from my own explanation?\n\nC3-AutoCoT modifies only the demo construction stage (clustering and downstream prompting remain Auto-CoT).\n\nFor each candidate demo question q:\n1) Re-derive (within-question self-consistency): sample m CoT chains on q → answers A(q). Compute r_sc = max_a freq(a in A(q))/m.\n2) Rephrase (paraphrase invariance): generate p paraphrases with the same LLM; filter paraphrases that drift by requiring the multiset of numbers in q_i to match q. For each q_i sample m chains → answers A(q_i). Compute r_pi = mean_i P(answer=a* | q_i).\n3) Reconstruct (cycle-consistency / grounding): Take a representative chain c* for q that yields a*. Ask the LLM to reconstruct the original word problem given c*: q_hat = f_reconstruct(c*). Compute grounding score r_cc = 0.5·cos_sim(emb(q_hat), emb(q)) + 0.5·I(numbers(q_hat)=numbers(q)).\n\nCombined demo reliability: r = r_sc · r_pi · r_cc.\nAccept demo if r ≥ τ and at least one paraphrase survives the number-preservation filter.",
  "experimental_setup": "Tasks/Datasets: SVAMP (fast arithmetic word problems), GSM8K (stronger but heavier)\n\nModels: google/flan-t5-base (primary), google/flan-t5-large (robustness)\n\nCompared methods (same k demos, same evaluation):\n1) Zero-shot-CoT (no demos)\n2) Auto-CoT (cluster + single chain + heuristics)\n3) RW-AutoCoT (self-consistency demo filter)\n4) PIR-AutoCoT (self-consistency + paraphrase invariance)\n5) C3-AutoCoT (proposed)\n\nProtocol:\n- Demo pool: first 500 train questions\n- Clustering: SentenceTransformer embeddings + k-means (k=8)\n- For each cluster, consider up to T=10 candidates until a demo is accepted\n- Evaluation: N=200 test questions; greedy decoding\n- Seeds: 3 seeds (k-means init + sampling)\n\nHyperparameters:\n- m=4 samples per variant\n- p=2 paraphrases\n- reconstruction samples=1 (deterministic)\n- τ=0.25–0.40",
  "primary_metric": "accuracy",
  "experimental_code": "import re, random\nimport numpy as np\nfrom collections import Counter\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\n\nnum_re = re.compile(r\"-?\\d+(?:\\.\\d+)?\")\n\ndef extract_numbers(s: str):\n    return num_re.findall(s)\n\ndef extract_final_number(text: str):\n    nums = extract_numbers(text)\n    return nums[-1] if nums else None\n\ndef agreement_rate(answers):\n    answers = [a for a in answers if a is not None]\n    if not answers:\n        return 0.0, None\n    c = Counter(answers)\n    maj, ct = c.most_common(1)[0]\n    return ct / len(answers), maj\n\n@torch.inference_mode()\ndef generate_texts(model, tok, prompt, n=1, do_sample=True, temperature=0.7, max_new_tokens=256):\n    inp = tok(prompt, return_tensors='pt', truncation=True).to(model.device)\n    outs = model.generate(\n        **inp,\n        do_sample=do_sample,\n        temperature=temperature if do_sample else None,\n        top_p=0.95 if do_sample else None,\n        num_return_sequences=n,\n        max_new_tokens=max_new_tokens,\n    )\n    return tok.batch_decode(outs, skip_special_tokens=True)\n\nif __name__ == \"__main__\":\n    print(\"C3-AutoCoT implementation scaffold\")\n",
  "expected_result": "On SVAMP (200-example test subset) with flan-t5-base, k=8 demos, m=4, p=2:\n- Auto-CoT: 0.20–0.28 accuracy\n- RW-AutoCoT: 0.23–0.32\n- PIR-AutoCoT: 0.27–0.36\n- C3-AutoCoT (proposed): 0.30–0.40\n\nExpected gains:\n- C3 vs Auto-CoT: +0.07 to +0.12\n- C3 vs PIR-AutoCoT: +0.02 to +0.05\n\nHigher is better.",
  "expected_conclusion": "C3-AutoCoT strengthens Auto-CoT with a cognitively inspired triple-check that better matches human verification: (1) re-derive (self-consistency), (2) rephrase (paraphrase invariance), and (3) reconstruct (cycle-consistency grounding). This targets a sharper and more socially relevant failure mode than PIR alone: demonstrations that are stable yet solve a different problem than asked."
}

## Experimental Design
{
  "experiment_summary": "Task: Solve elementary arithmetic word problems by generating a chain-of-thought (CoT) rationale and a final numeric answer.\n\nPurpose: Demonstrate that C3-AutoCoT yields higher downstream problem-solving accuracy than PIR-AutoCoT by filtering out \"plausible but ungrounded\" candidate demonstrations during the demo construction stage.\n\nCore workflow:\n1) Demo pool: first 500 SVAMP training questions\n2) Clustering: SentenceTransformer embeddings + k-means (k=8)\n3) Demo selection: For each cluster, accept candidate demo if r=r_sc·r_pi·r_cc≥τ\n4) Evaluation: 200 test questions with fixed 8-demo prompt, greedy decoding",
  "runner_config": {
    "runner_label": ["self-hosted", "gpu-runner"],
    "description": "NVIDIA H200, VRAM: 140 GB, RAM: 240 GB"
  },
  "evaluation_metrics": [
    {
      "name": "accuracy",
      "description": "End-task correctness: proportion of test questions with correct final numeric answer. Extract predicted answer as last number in output; compare with gold answer."
    },
    {
      "name": "demo_acceptance_rate",
      "description": "Fraction of clusters that successfully accepted a demo candidate within top-T attempts."
    },
    {
      "name": "mean_reliability_components",
      "description": "Average values of r_sc (self-consistency), r_pi (paraphrase invariance), r_cc (cycle-consistency grounding) across accepted demos."
    },
    {
      "name": "grounding_utility_correlation",
      "description": "Correlation between r_cc and downstream accuracy per cluster (does grounding predict usefulness?)."
    }
  ],
  "models_to_use": ["Qwen3-8B (8B parameters)"],
  "datasets_to_use": ["SVAMP (MU-NLPC/Calc-svamp)"],
  "proposed_method": {
    "method_name": "C3-AutoCoT (Cycle-Consistent & Paraphrase-Invariant Reliability Auto-CoT)",
    "description": "Inference-only enhancement to Auto-CoT demonstration construction. Estimate demo reliability along three label-free axes: (1) within-question self-consistency (r_sc), (2) paraphrase invariance with number-preservation filter (r_pi), and (3) cycle-consistency grounding (r_cc). Accept demo if r=r_sc·r_pi·r_cc≥τ."
  },
  "comparative_methods": [
    {
      "method_name": "PIR-AutoCoT (Paraphrase-Invariant Reliability Auto-CoT)",
      "description": "Baseline that filters demos based on (1) within-question self-consistency and (2) paraphrase invariance. Does not include cycle-consistency grounding. Reliability: r=r_sc·r_pi."
    }
  ]
}

## Code Generation Instructions
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method and experimental design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/runs/*.yaml` files
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)
- DATA LEAK PREVENTION: Model receives ONLY inputs during training/inference; labels used ONLY for loss computation, NEVER concatenated to inputs

## Safety & Reliability Standards
1. **Defensive Implementation**:
   - **Component Robustness**: Handle missing or invalid defaults explicitly (e.g., tokenizer `pad_token`, image normalization stats, or dataset-specific configs).
   - **Gradient Integrity**: Use `torch.autograd.grad(create_graph=False)` for auxiliary updates to protect main gradients.

2. **Critical Lifecycle Assertions**: Insert assertions at key stages in `train.py`:
   - **Post-Init**: Assert critical attributes are valid immediately after loading (e.g., tokenizer `pad_token_id`, model output dimensions).
   - **Batch-Start**: Assert input/label shapes match at the start of the loop (at least for step 0).
   - **Pre-Optimizer**: **CRITICAL:** Before `optimizer.step()`, assert that gradients exist (not None) and are not zero. This detects if custom logic accidentally erased gradients.

## Hydra Configuration Structure
Each run config file (`config/runs/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (proposed, comparative-1, etc.)
- model: Model-specific parameters
- dataset: Dataset-specific parameters
- training: Training hyperparameters
- optuna: Hyperparameter search space (if applicable)

## Command Line Interface
The generated code must support:

**Training (main.py):**
```bash
uv run python -u -m src.main run={run_id} results_dir={path} mode=full
uv run python -u -m src.main run={run_id} results_dir={path} mode=trial
```

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, ...)`
- Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
- Log ALL metrics to WandB comprehensively
- Save final/best metrics to WandB summary

**`src/evaluate.py`**: Independent evaluation and visualization script
- Parse command line arguments: `results_dir`, `run_ids` (JSON string list)
- Retrieve experimental data from WandB API for specified run_ids
- Per-run processing: Export run-specific metrics to `{results_dir}/{run_id}/metrics.json`
- Aggregated analysis: Export to `{results_dir}/comparison/aggregated_metrics.json`
- Generate comparison figures

**`src/preprocess.py`**: Complete preprocessing pipeline for specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods

**`src/main.py`**: Main orchestrator
- Use `@hydra.main(config_path="../config")`
- Implement mode-based configuration

**`config/config.yaml`**: Main Hydra configuration file
- Include WandB configuration
- WANDB_API_KEY environment variable is available

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb`, `torch`, `transformers`, `datasets`, `optuna` (if used)

## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Run Execution**: main.py executes a single run_id passed via CLI
4. **Completeness**: No simplified versions, no TODOs

Generate the complete code now.
